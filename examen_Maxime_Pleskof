1)	Décrivez les 3 composantes de l’écosystème Hadoop : Décrivez les succinctement, expliquez votre choix :
-	HDFS : Hadoop Distributed File System. Il s’agit du stockage primaire utilisé par les applications Hadoop. HDFS permet de stocker une grande quantité de données et permet l’abstraction du stockage physique, permettant de manipuler les fichiers comme s’ils se trouvaient sur un disque physique unique. Sans HDFS, Hadoop ne peut fonctionner.
-	MapReduce : permet la manipulation de grandes quantités de données en les distribuant dans un cluster de machine. MapReduce permet d’effectuer des calculs parallèles et distribués. Il est la clé de l’efficacité des traitements d’Hadoop.
-	YARN (Yet Another Resource Negotiator) : est une technologie de gestion des clusters permettant la gestion des ressources des clusters de façon centralisée et dynamique. Il permet par ailleurs à des moteurs de données multiples d’accéder simultanément à des données présentes sur Hadoop. YARN possède deux daemons « resource manager » et « application master».
Ces 3 éléments semblent être les composantes principales d’Hadoop dans la mesure où ils constituent le cœur de l’environnement : la gestion de très grandes quantités de données de façon efficace et sûre, et la gestion des ressources du cluster.


2)	HDFS est un système de fichier scalable, distribué et portable écrit en Java. Il est capable de stocker des éléments de plusieurs centaines de giga-octets de données sur de nombreux supports physiques comme s’il s’agissait d’un seul disque. Il permet par ailleurs la redondance des données en les répliquant au moins 3 fois.
Néanmoins, HDFS est peu adapté pour des opérations nécessitant des écritures concurrentes, ce qui peut être un point faible. 


3)	Spark est un framework de calcul distribué. Il s’ait d’un ensemble d’outils applicatifs permettant des analyses complexes à grande échelle. Il permet des calculs parallèles et est tolérant aux pannes. Il se base sur des RDD (resilient distributed dataset), stockés sur plusieurs machines. Spark est divisé en Spark Core, Spark SQL, Spark Streaming, MLlib et GraphX.  Spark présente des inconvénients, notamment des latences élevées et des problèmes de gestion de petits fichiers. Spark étant in memory, il consomme énormément de mémoire et peut être très couteux. 


4)	Spark peut en effet tourner sans HDFS. Néanmoins, n’ayant pas de mécanisme de stockage par défaut, si l’on souhaite stocker des données, il faudra faire appel à un système de fichiers comme HDFS ou Amazon S3. 
Spark peut en effet tourner sans Yarn, mais sera logiquement privé des fonctions de ce dernier, notamment le queuing. 


5)	Hadoop 1 ne supporte que le modèle de calcul de Map-Reduce, là où Hadoop 2 supporte d’autres modèles distribués comme Spark. La scalabilité de Hadoop 1 est limitée à 4000 nœuds par clusters, contre 10 000 nœuds par clusteurs pour Hadoop 2. Par ailleurs Hadoop 2 a introduit YARN, qui gère désormais la mémoire et le CPU, et permet ainsi une séparation nette entre gestion des ressources et traitement des données.
